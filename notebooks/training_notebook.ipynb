{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n",
            "CUDA available: False\n",
            "MPS available: True\n",
            "MPS fallback enabled: 1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Enable MPS fallback for unsupported operations\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from dataset import PreprocessedDataset\n",
        "from models import ViTClassifier\n",
        "from utils import (\n",
        "    MetricsCalculator,\n",
        "    compute_class_weights,\n",
        "    Trainer,\n",
        "    WeightedBCELoss\n",
        ")\n",
        "from utils.config_utils import load_config, load_env, get_device, print_config, validate_config\n",
        "from utils.data_analysis import analyze_dataset\n",
        "\n",
        "import wandb\n",
        "\n",
        "SEED = 5252\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "print(f\"MPS fallback enabled: {os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK', '0')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded environment variables from .env\n",
            "✓ Configuration validated successfully\n",
            "\n",
            "Configuration:\n",
            "model:\n",
            "  name: google/vit-base-patch16-224\n",
            "  pretrained: True\n",
            "  num_classes: 5\n",
            "  input_channels: 9\n",
            "  channel_adaptation: avg_pool\n",
            "  dropout: 0.1\n",
            "  backend: huggingface\n",
            "data:\n",
            "  data_root: processed_data\n",
            "  train_split: train\n",
            "  val_split: val\n",
            "  test_split: test\n",
            "  batch_size: 16\n",
            "  num_workers: 4\n",
            "  pin_memory: True\n",
            "training:\n",
            "  epochs: 1\n",
            "  learning_rate: 0.0001\n",
            "  weight_decay: 1e-05\n",
            "  optimizer: adamw\n",
            "  scheduler: cosine\n",
            "  warmup_epochs: 5\n",
            "  gradient_clip: 1.0\n",
            "loss:\n",
            "  type: weighted_bce\n",
            "  pos_weight_strategy: inverse_freq\n",
            "metrics:\n",
            "  track_per_class: True\n",
            "  metrics_list: ['auc_roc', 'precision', 'recall', 'f1_score', 'accuracy']\n",
            "  threshold: 0.5\n",
            "wandb:\n",
            "  project: Deep Machine Learning Project\n",
            "  entity: None\n",
            "  log_interval: 10\n",
            "  log_model: True\n",
            "  watch_model: True\n",
            "checkpoint:\n",
            "  save_dir: checkpoints\n",
            "  save_frequency: 5\n",
            "  save_best: True\n",
            "  metric_for_best: val_auc_roc_macro\n",
            "  mode: max\n",
            "hardware:\n",
            "  device: auto\n",
            "  seed: 5252\n",
            "Using MPS (Apple Silicon) device\n",
            "\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables and config\n",
        "env_vars = load_env()\n",
        "config = load_config('../configs/base_config.yaml')\n",
        "validate_config(config)\n",
        "\n",
        "print(\"\\nConfiguration:\")\n",
        "print_config(config)\n",
        "\n",
        "# Get device\n",
        "device = get_device(config['hardware']['device'])\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes:\n",
            "  Train: 1828 samples\n",
            "  Val:   228 samples\n",
            "  Test:  229 samples\n",
            "\n",
            "DataLoaders created:\n",
            "  Train batches: 115\n",
            "  Val batches:   15\n",
            "  Test batches:  15\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "data_root = config['data']['data_root']\n",
        "\n",
        "train_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'train'))\n",
        "val_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'val'))\n",
        "test_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'test'))\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val:   {len(val_dataset)} samples\")\n",
        "print(f\"  Test:  {len(test_dataset)} samples\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['data']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=config['data']['num_workers'],\n",
        "    pin_memory=config['data']['pin_memory']\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['data']['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=config['data']['num_workers'],\n",
        "    pin_memory=config['data']['pin_memory']\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config['data']['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=config['data']['num_workers'],\n",
        "    pin_memory=config['data']['pin_memory']\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches:   {len(val_loader)}\")\n",
        "print(f\"  Test batches:  {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing class weights from dataset...\n",
            "\n",
            "Class Distribution:\n",
            "------------------------------------------------------------\n",
            "epidural            :    87 pos ( 4.76%),  1741 neg, weight: 20.0115\n",
            "intraparenchymal    :   449 pos (24.56%),  1379 neg, weight: 3.0713\n",
            "intraventricular    :   398 pos (21.77%),  1430 neg, weight: 3.5930\n",
            "subarachnoid        :   515 pos (28.17%),  1313 neg, weight: 2.5495\n",
            "subdural            :   470 pos (25.71%),  1358 neg, weight: 2.8894\n",
            "------------------------------------------------------------\n",
            "\n",
            "Computed class weights:\n",
            "  epidural            : 20.0115\n",
            "  intraparenchymal    : 3.0713\n",
            "  intraventricular    : 3.5930\n",
            "  subarachnoid        : 2.5495\n",
            "  subdural            : 2.8894\n"
          ]
        }
      ],
      "source": [
        "# Compute class weights from training data\n",
        "class_weights = compute_class_weights(\n",
        "    train_dataset,\n",
        "    strategy=config['loss']['pos_weight_strategy']\n",
        ")\n",
        "\n",
        "print(f\"\\nComputed class weights:\")\n",
        "class_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
        "for name, weight in zip(class_names, class_weights):\n",
        "    print(f\"  {name:20s}: {weight:.4f}\")\n",
        "\n",
        "# Move to device\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: google/vit-base-patch16-224\n",
            "Backend: huggingface\n",
            "Total parameters: 85,802,501\n",
            "Trainable parameters: 3,845 (backbone frozen)\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "model = ViTClassifier(\n",
        "    model_name=config['model']['name'],\n",
        "    num_classes=config['model']['num_classes'],\n",
        "    pretrained=config['model']['pretrained'],\n",
        "    input_channels=config['model']['input_channels'],\n",
        "    channel_adaptation=config['model']['channel_adaptation'],\n",
        "    dropout=config['model']['dropout'],\n",
        "    backend=config['model'].get('backend', 'huggingface')\n",
        ")\n",
        "\n",
        "model.freeze_backbone()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Print model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel: {config['model']['name']}\")\n",
        "print(f\"Backend: {config['model'].get('backend', 'huggingface')}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} (backbone frozen)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Weighted Binary Cross Entropy Loss\n",
            "Using Cosine Annealing LR Scheduler\n",
            "\n",
            "Optimizer: AdamW\n",
            "  Learning rate: 0.0001\n",
            "  Weight decay: 1e-05\n"
          ]
        }
      ],
      "source": [
        "# Loss function\n",
        "criterion = WeightedBCELoss(pos_weights=class_weights)\n",
        "print(\"Using Weighted Binary Cross Entropy Loss\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['training']['learning_rate'],\n",
        "    weight_decay=config['training']['weight_decay']\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "if config['training']['scheduler'] == 'cosine':\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=config['training']['epochs'],\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "    print(\"Using Cosine Annealing LR Scheduler\")\n",
        "elif config['training']['scheduler'] == 'step':\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=10,\n",
        "        gamma=0.1\n",
        "    )\n",
        "    print(\"Using Step LR Scheduler\")\n",
        "else:\n",
        "    scheduler = None\n",
        "    print(\"No LR Scheduler\")\n",
        "\n",
        "print(f\"\\nOptimizer: AdamW\")\n",
        "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  Weight decay: {config['training']['weight_decay']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahmv59\u001b[0m (\u001b[33mdml_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/noahmv/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.22.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/noahmv/Desktop/noah/studies/Master's/Chalmers/2nd year/sp_1/DML/Project/notebooks/wandb/run-20251015_231510-i6r9ei35</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project/runs/i6r9ei35' target=\"_blank\">vit_ich_training</a></strong> to <a href='https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project' target=\"_blank\">https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project/runs/i6r9ei35' target=\"_blank\">https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project/runs/i6r9ei35</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ W&B initialized: vit_ich_training\n",
            "  Project: Deep Machine Learning Project\n",
            "  Run URL: https://wandb.ai/dml_project/Deep%20Machine%20Learning%20Project/runs/i6r9ei35\n"
          ]
        }
      ],
      "source": [
        "# Initialize W&B\n",
        "USE_WANDB = True \n",
        "\n",
        "if USE_WANDB:\n",
        "    # Login to W&B (if not already logged in)\n",
        "    if env_vars.get('WANDB_API_KEY'):\n",
        "        wandb.login(key=env_vars['WANDB_API_KEY'])\n",
        "    \n",
        "    # Initialize run\n",
        "    run = wandb.init(\n",
        "        project=config['wandb']['project'],\n",
        "        entity=env_vars.get('WANDB_ENTITY'),\n",
        "        name=env_vars.get('NOTEBOOK_NAME', 'vit_ich_training'),\n",
        "        config=config,\n",
        "        tags=['vit', 'ich', 'multi-label', 'transformer']\n",
        "    )\n",
        "    \n",
        "    # Watch model (log gradients and parameters)\n",
        "    if config['wandb']['watch_model']:\n",
        "        wandb.watch(model, log='all', log_freq=100)\n",
        "    \n",
        "    print(f\"✓ W&B initialized: {wandb.run.name}\")\n",
        "    print(f\"  Project: {config['wandb']['project']}\")\n",
        "    print(f\"  Run URL: {wandb.run.get_url()}\")\n",
        "else:\n",
        "    print(\"W&B disabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Starting Training for 1 epochs\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 115/115 [01:13<00:00,  1.57it/s, loss=1.05] \n",
            "Epoch 1 [Val]: 100%|██████████| 15/15 [01:49<00:00,  7.32s/it, loss=1.42]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/1\n",
            "  Train Loss: 1.0977 | Train AUC: 0.5410 | Train F1: 0.3074\n",
            "  Val Loss:   1.1261 | Val AUC:   0.5609 | Val F1:   0.3236\n",
            "  Saved checkpoint: checkpoints/checkpoint_epoch_1.pt\n",
            "  ✓ New best model saved: checkpoints/best_model.pt\n",
            "\n",
            "================================================================================\n",
            "Training Complete!\n",
            "Best val_auc_roc_macro: 0.5609\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    config=config,\n",
        "    use_wandb=USE_WANDB\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(epochs=config['training']['epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best checkpoint\n",
        "best_checkpoint_path = Path('../checkpoints/best_model.pt')\n",
        "\n",
        "if best_checkpoint_path.exists():\n",
        "    print(f\"Loading best model from {best_checkpoint_path}\")\n",
        "    checkpoint = trainer.load_checkpoint(best_checkpoint_path)\n",
        "    print(f\"  Best validation {trainer.best_metric_name}: {checkpoint['best_metric']:.4f}\")\n",
        "else:\n",
        "    print(\"No checkpoint found, using current model\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics = trainer.test(test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar chart of per-class metrics\n",
        "class_names_plot = ['Epidural', 'Intraparenchymal', 'Intraventricular', 'Subarachnoid', 'Subdural']\n",
        "class_names_key = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
        "\n",
        "# Extract metrics\n",
        "precisions = [test_metrics[f'precision_{name}'] for name in class_names_key]\n",
        "recalls = [test_metrics[f'recall_{name}'] for name in class_names_key]\n",
        "f1_scores = [test_metrics[f'f1_{name}'] for name in class_names_key]\n",
        "aucs = [test_metrics.get(f'auc_roc_{name}', 0) for name in class_names_key]\n",
        "\n",
        "# Plot\n",
        "x = np.arange(len(class_names_plot))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.bar(x - 1.5*width, precisions, width, label='Precision', color='#3498db')\n",
        "ax.bar(x - 0.5*width, recalls, width, label='Recall', color='#e74c3c')\n",
        "ax.bar(x + 0.5*width, f1_scores, width, label='F1-Score', color='#2ecc71')\n",
        "ax.bar(x + 1.5*width, aucs, width, label='AUC-ROC', color='#9b59b6')\n",
        "\n",
        "ax.set_xlabel('Hemorrhage Type', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(class_names_plot, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../checkpoints/per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()bd5ba8e016b3d0ee9edec60637d209cc6308e6f0\n",
        "\n",
        "# Log to W&B\n",
        "if USE_WANDB:\n",
        "    wandb.log({\"per_class_metrics\": wandb.Image(plt)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save test metrics to JSON\n",
        "results = {\n",
        "    'model': config['model']['name'],\n",
        "    'test_metrics': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
        "                     for k, v in test_metrics.items()},\n",
        "    'config': config\n",
        "}\n",
        "\n",
        "results_path = Path('../checkpoints/test_results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"✓ Results saved to {results_path}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOverall Performance:\")\n",
        "print(f\"  AUC-ROC (Macro):      {test_metrics['auc_roc_macro']:.4f}\")\n",
        "print(f\"  AUC-ROC (Weighted):   {test_metrics.get('auc_roc_weighted', 0):.4f}\")\n",
        "print(f\"  F1-Score (Macro):     {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"  Exact Match Accuracy: {test_metrics['accuracy_exact']:.4f}\")\n",
        "print(f\"  Hamming Accuracy:     {test_metrics['accuracy_hamming']:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Finish W&B run\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "    print(\"✓ W&B run finished\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tfg-noah",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
