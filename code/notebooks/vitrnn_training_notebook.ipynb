{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/chalmers/users/felixbj/Programming/deep-ml/Deep-Machine-Learning-Project/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/chalmers/users/felixbj/Programming/deep-ml/Deep-Machine-Learning-Project/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "MPS available: False\n",
      "MPS fallback enabled: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from dataset import PreprocessedDataset\n",
    "from models import ViTTripletBiRNNClassifier\n",
    "from utils import (\n",
    "    MetricsCalculator,\n",
    "    compute_class_weights,\n",
    "    Trainer,\n",
    "    WeightedBCELoss\n",
    ")\n",
    "from utils.config_utils import load_config, load_env, get_device, print_config, validate_config\n",
    "from utils.data_analysis import analyze_dataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "SEED = 5252\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS fallback enabled: {os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK', '0')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: .env file not found. W&B API key should be set manually.\n",
      "✓ Configuration validated successfully\n",
      "\n",
      "Configuration:\n",
      "model:\n",
      "  name: ../pre-trained-model\n",
      "  pretrained: True\n",
      "  num_classes: 5\n",
      "  input_channels: 9\n",
      "  slice_channels: 3\n",
      "  dropout: 0.1\n",
      "  backend: huggingface\n",
      "  rnn_hidden_size: 512\n",
      "  rnn_num_layers: 1\n",
      "  rnn_dropout: 0.0\n",
      "  sequence_pooling: last\n",
      "data:\n",
      "  data_root: processed_data\n",
      "  train_split: train\n",
      "  val_split: val\n",
      "  test_split: test\n",
      "  batch_size: 128\n",
      "  num_workers: 12\n",
      "  pin_memory: True\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 1e-05\n",
      "  optimizer: adamw\n",
      "  scheduler: cosine\n",
      "  warmup_epochs: 5\n",
      "  gradient_clip: 1.0\n",
      "  early_stopping:\n",
      "    enabled: True\n",
      "    patience: 10\n",
      "    min_delta: 0.0\n",
      "    mode: max\n",
      "loss:\n",
      "  type: weighted_bce\n",
      "  pos_weight_strategy: inverse_freq\n",
      "metrics:\n",
      "  track_per_class: True\n",
      "  metrics_list: ['auc_roc', 'precision', 'recall', 'f1_score', 'accuracy']\n",
      "  threshold: 0.5\n",
      "wandb:\n",
      "  project: Deep Machine Learning Project\n",
      "  entity: None\n",
      "  log_interval: 10\n",
      "  log_model: True\n",
      "  watch_model: True\n",
      "checkpoint:\n",
      "  save_dir: checkpoints\n",
      "  save_frequency: 5\n",
      "  save_best: True\n",
      "  metric_for_best: val_auc_roc_macro\n",
      "  mode: max\n",
      "hardware:\n",
      "  device: auto\n",
      "  seed: 5252\n",
      "Using CUDA device: NVIDIA GeForce RTX 3070\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and config\n",
    "env_vars = load_env()\n",
    "config = load_config('../configs/vitrnn_base_config.yaml')\n",
    "validate_config(config)\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print_config(config)\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['hardware']['device'])\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "  Train: 1828 samples\n",
      "  Val:   228 samples\n",
      "  Test:  229 samples\n",
      "\n",
      "DataLoaders created:\n",
      "  Train batches: 15\n",
      "  Val batches:   2\n",
      "  Test batches:  2\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "data_root = config['data']['data_root']\n",
    "\n",
    "train_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'train'))\n",
    "val_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'val'))\n",
    "test_dataset = PreprocessedDataset(root_dir=os.path.join('..', data_root, 'test'))\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['data']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    pin_memory=config['data']['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['data']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    pin_memory=config['data']['pin_memory']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['data']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    pin_memory=config['data']['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights from dataset...\n",
      "\n",
      "Class Distribution:\n",
      "------------------------------------------------------------\n",
      "epidural            :    90 pos ( 4.92%),  1738 neg, weight: 19.3111\n",
      "intraparenchymal    :   447 pos (24.45%),  1381 neg, weight: 3.0895\n",
      "intraventricular    :   400 pos (21.88%),  1428 neg, weight: 3.5700\n",
      "subarachnoid        :   515 pos (28.17%),  1313 neg, weight: 2.5495\n",
      "subdural            :   465 pos (25.44%),  1363 neg, weight: 2.9312\n",
      "------------------------------------------------------------\n",
      "\n",
      "Computed class weights:\n",
      "  epidural            : 19.3111\n",
      "  intraparenchymal    : 3.0895\n",
      "  intraventricular    : 3.5700\n",
      "  subarachnoid        : 2.5495\n",
      "  subdural            : 2.9312\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights from training data\n",
    "class_weights = compute_class_weights(\n",
    "    train_dataset,\n",
    "    strategy=config['loss']['pos_weight_strategy']\n",
    ")\n",
    "\n",
    "print(f\"\\nComputed class weights:\")\n",
    "class_names = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "for name, weight in zip(class_names, class_weights):\n",
    "    print(f\"  {name:20s}: {weight:.4f}\")\n",
    "\n",
    "# Move to device\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at ../pre-trained-model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: ../pre-trained-model\n",
      "Backend: huggingface\n",
      "Slices per sample: 3\n",
      "Total parameters: 91,647,493\n",
      "Trainable parameters: 5,258,245 (ViT backbone frozen)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = ViTTripletBiRNNClassifier(\n",
    "    model_name=config['model']['name'],\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    pretrained=config['model']['pretrained'],\n",
    "    input_channels=config['model']['input_channels'],\n",
    "    slice_channels=config['model'].get('slice_channels', 3),\n",
    "    dropout=config['model']['dropout'],\n",
    "    rnn_hidden_size=config['model'].get('rnn_hidden_size', 512),\n",
    "    rnn_num_layers=config['model'].get('rnn_num_layers', 1),\n",
    "    rnn_dropout=config['model'].get('rnn_dropout', 0.0),\n",
    "    sequence_pooling=config['model'].get('sequence_pooling', 'last'),\n",
    "    backend=config['model'].get('backend', 'huggingface')\n",
    ")\n",
    "\n",
    "model.freeze_backbone()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel: {config['model']['name']}\")\n",
    "print(f\"Backend: {config['model'].get('backend', 'huggingface')}\")\n",
    "print(f\"Slices per sample: {model.num_slices}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} (ViT backbone frozen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Weighted Binary Cross Entropy Loss\n",
      "Using Cosine Annealing LR Scheduler\n",
      "\n",
      "Optimizer: AdamW\n",
      "  Learning rate: 0.0001\n",
      "  Weight decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = WeightedBCELoss(pos_weights=class_weights)\n",
    "print(\"Using Weighted Binary Cross Entropy Loss\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config['training']['scheduler'] == 'cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['training']['epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    print(\"Using Cosine Annealing LR Scheduler\")\n",
    "elif config['training']['scheduler'] == 'step':\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=10,\n",
    "        gamma=0.1\n",
    "    )\n",
    "    print(\"Using Step LR Scheduler\")\n",
    "else:\n",
    "    scheduler = None\n",
    "    print(\"No LR Scheduler\")\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Weight decay: {config['training']['weight_decay']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelixbj\u001b[0m (\u001b[33mfelixbj-chalmers-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/chalmers/users/felixbj/Programming/deep-ml/Deep-Machine-Learning-Project/notebooks/wandb/run-20251020_123112-moou6gfw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw' target=\"_blank\">vit_ich_training</a></strong> to <a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project' target=\"_blank\">https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw' target=\"_blank\">https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ W&B initialized: vit_ich_training\n",
      "  Project: Deep Machine Learning Project\n",
      "  Run URL: https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw\n"
     ]
    }
   ],
   "source": [
    "# Initialize W&B\n",
    "USE_WANDB = True \n",
    "\n",
    "if USE_WANDB:\n",
    "    # Login to W&B (if not already logged in)\n",
    "    if env_vars.get('WANDB_API_KEY'):\n",
    "        wandb.login(key=env_vars['WANDB_API_KEY'])\n",
    "    \n",
    "    # Initialize run\n",
    "    run = wandb.init(\n",
    "        project=config['wandb']['project'],\n",
    "        entity=env_vars.get('WANDB_ENTITY'),\n",
    "        name=env_vars.get('NOTEBOOK_NAME', 'vit_ich_training'),\n",
    "        config=config,\n",
    "        tags=['vit', 'ich', 'multi-label', 'transformer']\n",
    "    )\n",
    "    \n",
    "    # Watch model (log gradients and parameters)\n",
    "    if config['wandb']['watch_model']:\n",
    "        wandb.watch(model, log='all', log_freq=100)\n",
    "    \n",
    "    print(f\"✓ W&B initialized: {wandb.run.name}\")\n",
    "    print(f\"  Project: {config['wandb']['project']}\")\n",
    "    print(f\"  Run URL: {wandb.run.get_url()}\")\n",
    "else:\n",
    "    print(\"W&B disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    use_wandb=USE_WANDB\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(epochs=config['training']['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to checkpoints/test_results.json\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Overall Performance:\n",
      "  AUC-ROC (Macro):      0.8886\n",
      "  AUC-ROC (Weighted):   0.8956\n",
      "  F1-Score (Macro):     0.6554\n",
      "  Exact Match Accuracy: 0.5153\n",
      "  Hamming Accuracy:     0.8550\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy_exact</td><td>▁</td></tr><tr><td>test/accuracy_hamming</td><td>▁</td></tr><tr><td>test/auc_roc_epidural</td><td>▁</td></tr><tr><td>test/auc_roc_intraparenchymal</td><td>▁</td></tr><tr><td>test/auc_roc_intraventricular</td><td>▁</td></tr><tr><td>test/auc_roc_macro</td><td>▁</td></tr><tr><td>test/auc_roc_micro</td><td>▁</td></tr><tr><td>test/auc_roc_subarachnoid</td><td>▁</td></tr><tr><td>test/auc_roc_subdural</td><td>▁</td></tr><tr><td>test/auc_roc_weighted</td><td>▁</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy_exact</td><td>0.51528</td></tr><tr><td>test/accuracy_hamming</td><td>0.85502</td></tr><tr><td>test/auc_roc_epidural</td><td>0.84028</td></tr><tr><td>test/auc_roc_intraparenchymal</td><td>0.89878</td></tr><tr><td>test/auc_roc_intraventricular</td><td>0.95159</td></tr><tr><td>test/auc_roc_macro</td><td>0.88857</td></tr><tr><td>test/auc_roc_micro</td><td>0.90699</td></tr><tr><td>test/auc_roc_subarachnoid</td><td>0.85809</td></tr><tr><td>test/auc_roc_subdural</td><td>0.89412</td></tr><tr><td>test/auc_roc_weighted</td><td>0.89558</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vit_ich_training</strong> at: <a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw' target=\"_blank\">https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project/runs/moou6gfw</a><br> View project at: <a href='https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project' target=\"_blank\">https://wandb.ai/felixbj-chalmers-university-of-technology/Deep%20Machine%20Learning%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251020_123112-moou6gfw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ W&B run finished\n"
     ]
    }
   ],
   "source": [
    "# Save test metrics to JSON\n",
    "results = {\n",
    "    'model': config['model']['name'],\n",
    "    'test_metrics': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                     for k, v in test_metrics.items()},\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "results_path = Path('./checkpoints/test_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to {results_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  AUC-ROC (Macro):      {test_metrics['auc_roc_macro']:.4f}\")\n",
    "print(f\"  AUC-ROC (Weighted):   {test_metrics.get('auc_roc_weighted', 0):.4f}\")\n",
    "print(f\"  F1-Score (Macro):     {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  Exact Match Accuracy: {test_metrics['accuracy_exact']:.4f}\")\n",
    "print(f\"  Hamming Accuracy:     {test_metrics['accuracy_hamming']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Finish W&B run\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"✓ W&B run finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg-noah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
