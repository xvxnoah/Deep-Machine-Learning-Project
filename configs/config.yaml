# Model Configuration
# Multi-label brain hemorrhage classification with Vision Transformers

model:
  name: "google/vit-base-patch16-224"
  pretrained: true
  num_classes: 5
  input_channels: 9
  channel_adaptation: "deep_conv"
  dropout: 0.35  # Increased dropout for better generalization
  backend: "huggingface"
  
  # Enhanced Classification Head
  head_type: "attention_mlp"
  head_hidden_dims: [512, 256, 128]
  head_num_attention_heads: 8
  head_attention_dropout: 0.25  # Increased from 0.2
  head_use_residual: true

# Data Configuration
data:
  data_root: "DL_Project_Processed_Data"
  train_split: "train"
  val_split: "val"
  test_split: "test"
  batch_size: 16
  num_workers: 0
  pin_memory: false
  
  # Advanced Sampling Strategy
  sampler:
    type: "imbalanced"  # Options: none, weighted, imbalanced, balanced_batch
    strategy: "sqrt_inverse_freq"  # For imbalanced sampler
    rare_class_boost: 2.5  # Boost epidural and subarachnoid by 2.5x
    num_samples: null  # null = dataset length per epoch

# Augmentation Configuration
augmentation:
  # Spatial augmentations
  mode: "standard"
  rotation_degrees: 15  # Increased from 10
  translate: [0.15, 0.15]  # Increased from 0.1
  scale_range: [0.85, 1.15]  # Wider range
  horizontal_flip: true
  vertical_flip: false
  intensity_shift: 0.15  # Increased
  intensity_scale: 0.15  # Increased
  noise_std: 0.02  # Increased
  aug_prob: 0.6  # Increased from 0.5
  
  # Mixup/CutMix - Critical for generalization
  use_mixup: true
  use_cutmix: true
  mixup_alpha: 0.3  # Increased from 0.2
  cutmix_alpha: 1.0
  mixup_cutmix_prob: 0.6  # Increased probability
  
  # Random erasing
  use_random_erasing: true
  random_erasing_prob: 0.3  # Increased

# Training Configuration
training:
  epochs: 100  # Increased from 50 - early stopping will handle
  learning_rate: 0.0003  # Increased from 0.0001 (3x)
  weight_decay: 0.01
  optimizer: "adamw"
  
  # Advanced Scheduler - Cosine with Warm Restarts for escaping plateaus
  scheduler: "cosine_warm_restarts"  # Options: cosine, cosine_warm_restarts, onecycle, plateau
  warmup_epochs: 8  # Increased from 5
  warmup_lr_init: 1e-7  # Very low initial LR
  min_lr: 1e-7  # Minimum LR
  
  # Cosine Warm Restarts specific
  restart_period: 15  # Restart every 15 epochs
  restart_mult: 2  # Double the period after each restart
  
  # OneCycle specific (if using onecycle)
  onecycle_pct_start: 0.3
  onecycle_div_factor: 25.0
  onecycle_final_div: 10000.0
  
  # Plateau specific (if using plateau)
  plateau_factor: 0.5
  plateau_patience: 7
  
  gradient_clip: 1.0
  accumulation_steps: 2
  
  unfreeze:
    enabled: false
    start_epoch: 20

# Enhanced Loss Configuration
loss:
  type: "combined_advanced"  # New advanced combined loss
  
  # Loss component weights - Optimized for rare classes
  loss_weights:
    bce: 0.4  # Reduced
    focal: 0.3
    asymmetric: 0.3  # Added asymmetric loss
  
  # BCE Configuration
  pos_weight_strategy: "sqrt_inverse_freq"
  
  # Focal Loss Configuration - Per-class alpha for rare classes
  focal_alpha: 0.25
  focal_gamma: 2.5  # Increased from 2.0
  per_class_focal_alpha: [0.75, 0.25, 0.20, 0.65, 0.30]  # [epidural, intra, intra_v, sub_a, sub_d]
  
  # Asymmetric Loss Configuration - Aggressive for rare classes
  asymmetric_gamma_neg: 5  # Increased from 4
  asymmetric_gamma_pos: 0.5  # Reduced from 1
  per_class_gamma_neg: [6, 4, 3, 5, 4]  # Higher for epidural and subarachnoid
  per_class_gamma_pos: [0.25, 0.5, 0.5, 0.4, 0.5]  # Lower for rare classes
  
  # Label smoothing
  label_smoothing: 0.08  # Increased from 0.05

# Early Stopping Configuration
early_stopping:
  enabled: true
  patience: 20  # Increased from 10 for warm restarts
  min_delta: 0.0005  # Reduced minimum delta
  restore_best_weights: true

# Metrics Configuration
metrics:
  track_per_class: true
  metrics_list:
    - "auc_roc"
    - "precision"
    - "recall"
    - "f1_score"
    - "accuracy"
  threshold: 0.5  # Default, will be optimized
  
  # Per-class threshold optimization
  optimize_thresholds: true
  threshold_optimization_method: "f1"  # Optimize for F1 score
  per_class_thresholds: true  # Optimize each class independently

# Evaluation Configuration
evaluation:
  # Advanced TTA for robust predictions
  tta:
    enabled: true
    strategy: "standard"  # Options: light, standard, aggressive
    combine_method: "mean"  # Options: mean, median, vote
    use_multi_scale: false
    scales: [0.95, 1.0, 1.05]  # If multi_scale enabled

# W&B Configuration
wandb:
  project: "Deep Machine Learning Project"
  entity: null
  log_interval: 10
  log_model: true
  watch_model: true

# Checkpoint Configuration
checkpoint:
  save_dir: "checkpoints"
  save_frequency: 10
  save_best: true
  metric_for_best: "val_f1_macro"  # Changed to F1 (more balanced)
  mode: "max"

# Hardware Configuration
hardware:
  device: "auto"
  seed: 5252

# Advanced Features
advanced:
  # Threshold optimization on validation set
  optimize_val_thresholds: true
  optimize_every_n_epochs: 5
  
  # Gradient checkpointing (if memory constrained)
  use_gradient_checkpointing: false
  
  # Mixed precision training
  use_amp: false
  
  # Exponential Moving Average of weights
  use_ema: false
  ema_decay: 0.999

